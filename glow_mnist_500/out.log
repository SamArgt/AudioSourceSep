TensorFlow version: 2.2.0
Eager execution: True
Glow Bijector 2 Blocks: K = 6 ; ShiftAndLogScaleResNet; n_filters = 64
flow sample shape:  (1, 28, 28, 1)
____________________________________________________________________________________________________
Network 	 Layer 	 Shape 						 #parameters
====================================================================================================
glowBlock1/glowStep0
	 ActNorm 		 scale (4,) | shift (4,) 		 8
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net
	 conv2d 		 kernel (3, 3, 4, 64) | bias (64,) 		 2368
	 batch_normalization 		 gamma (64,) | beta (64,) 		 128
	 conv2d_1 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_1 		 gamma (64,) | beta (64,) 		 128
	 conv2d_2 		 kernel (3, 3, 64, 8) | bias (8,) 		 4616
____________________________________________________________________________________________________
glowBlock1/glowStep0
	 inv1x1conv 		 L (4, 4) | log_S (4,) | U (4, 4) 		 36
____________________________________________________________________________________________________
glowBlock1/glowStep1
	 ActNorm 		 scale (4,) | shift (4,) 		 8
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_1
	 conv2d_3 		 kernel (3, 3, 4, 64) | bias (64,) 		 2368
	 batch_normalization_2 		 gamma (64,) | beta (64,) 		 128
	 conv2d_4 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_3 		 gamma (64,) | beta (64,) 		 128
	 conv2d_5 		 kernel (3, 3, 64, 8) | bias (8,) 		 4616
____________________________________________________________________________________________________
glowBlock1/glowStep1
	 inv1x1conv 		 L (4, 4) | log_S (4,) | U (4, 4) 		 36
____________________________________________________________________________________________________
glowBlock1/glowStep2
	 ActNorm 		 scale (4,) | shift (4,) 		 8
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_2
	 conv2d_6 		 kernel (3, 3, 4, 64) | bias (64,) 		 2368
	 batch_normalization_4 		 gamma (64,) | beta (64,) 		 128
	 conv2d_7 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_5 		 gamma (64,) | beta (64,) 		 128
	 conv2d_8 		 kernel (3, 3, 64, 8) | bias (8,) 		 4616
____________________________________________________________________________________________________
glowBlock1/glowStep2
	 inv1x1conv 		 L (4, 4) | log_S (4,) | U (4, 4) 		 36
____________________________________________________________________________________________________
glowBlock1/glowStep3
	 ActNorm 		 scale (4,) | shift (4,) 		 8
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_3
	 conv2d_9 		 kernel (3, 3, 4, 64) | bias (64,) 		 2368
	 batch_normalization_6 		 gamma (64,) | beta (64,) 		 128
	 conv2d_10 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_7 		 gamma (64,) | beta (64,) 		 128
	 conv2d_11 		 kernel (3, 3, 64, 8) | bias (8,) 		 4616
____________________________________________________________________________________________________
glowBlock1/glowStep3
	 inv1x1conv 		 L (4, 4) | log_S (4,) | U (4, 4) 		 36
____________________________________________________________________________________________________
glowBlock1/glowStep4
	 ActNorm 		 scale (4,) | shift (4,) 		 8
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_4
	 conv2d_12 		 kernel (3, 3, 4, 64) | bias (64,) 		 2368
	 batch_normalization_8 		 gamma (64,) | beta (64,) 		 128
	 conv2d_13 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_9 		 gamma (64,) | beta (64,) 		 128
	 conv2d_14 		 kernel (3, 3, 64, 8) | bias (8,) 		 4616
____________________________________________________________________________________________________
glowBlock1/glowStep4
	 inv1x1conv 		 L (4, 4) | log_S (4,) | U (4, 4) 		 36
____________________________________________________________________________________________________
glowBlock1/glowStep5
	 ActNorm 		 scale (4,) | shift (4,) 		 8
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_5
	 conv2d_15 		 kernel (3, 3, 4, 64) | bias (64,) 		 2368
	 batch_normalization_10 		 gamma (64,) | beta (64,) 		 128
	 conv2d_16 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_11 		 gamma (64,) | beta (64,) 		 128
	 conv2d_17 		 kernel (3, 3, 64, 8) | bias (8,) 		 4616
____________________________________________________________________________________________________
glowBlock1/glowStep5
	 inv1x1conv 		 L (4, 4) | log_S (4,) | U (4, 4) 		 36
____________________________________________________________________________________________________
glowBlock2/glowStep0
	 ActNorm 		 scale (8,) | shift (8,) 		 16
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_6
	 conv2d_18 		 kernel (3, 3, 8, 64) | bias (64,) 		 4672
	 batch_normalization_12 		 gamma (64,) | beta (64,) 		 128
	 conv2d_19 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_13 		 gamma (64,) | beta (64,) 		 128
	 conv2d_20 		 kernel (3, 3, 64, 16) | bias (16,) 		 9232
____________________________________________________________________________________________________
glowBlock2/glowStep0
	 inv1x1conv 		 L (8, 8) | log_S (8,) | U (8, 8) 		 136
____________________________________________________________________________________________________
glowBlock2/glowStep1
	 ActNorm 		 scale (8,) | shift (8,) 		 16
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_7
	 conv2d_21 		 kernel (3, 3, 8, 64) | bias (64,) 		 4672
	 batch_normalization_14 		 gamma (64,) | beta (64,) 		 128
	 conv2d_22 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_15 		 gamma (64,) | beta (64,) 		 128
	 conv2d_23 		 kernel (3, 3, 64, 16) | bias (16,) 		 9232
____________________________________________________________________________________________________
glowBlock2/glowStep1
	 inv1x1conv 		 L (8, 8) | log_S (8,) | U (8, 8) 		 136
____________________________________________________________________________________________________
glowBlock2/glowStep2
	 ActNorm 		 scale (8,) | shift (8,) 		 16
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_8
	 conv2d_24 		 kernel (3, 3, 8, 64) | bias (64,) 		 4672
	 batch_normalization_16 		 gamma (64,) | beta (64,) 		 128
	 conv2d_25 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_17 		 gamma (64,) | beta (64,) 		 128
	 conv2d_26 		 kernel (3, 3, 64, 16) | bias (16,) 		 9232
____________________________________________________________________________________________________
glowBlock2/glowStep2
	 inv1x1conv 		 L (8, 8) | log_S (8,) | U (8, 8) 		 136
____________________________________________________________________________________________________
glowBlock2/glowStep3
	 ActNorm 		 scale (8,) | shift (8,) 		 16
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_9
	 conv2d_27 		 kernel (3, 3, 8, 64) | bias (64,) 		 4672
	 batch_normalization_18 		 gamma (64,) | beta (64,) 		 128
	 conv2d_28 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_19 		 gamma (64,) | beta (64,) 		 128
	 conv2d_29 		 kernel (3, 3, 64, 16) | bias (16,) 		 9232
____________________________________________________________________________________________________
glowBlock2/glowStep3
	 inv1x1conv 		 L (8, 8) | log_S (8,) | U (8, 8) 		 136
____________________________________________________________________________________________________
glowBlock2/glowStep4
	 ActNorm 		 scale (8,) | shift (8,) 		 16
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_10
	 conv2d_30 		 kernel (3, 3, 8, 64) | bias (64,) 		 4672
	 batch_normalization_20 		 gamma (64,) | beta (64,) 		 128
	 conv2d_31 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_21 		 gamma (64,) | beta (64,) 		 128
	 conv2d_32 		 kernel (3, 3, 64, 16) | bias (16,) 		 9232
____________________________________________________________________________________________________
glowBlock2/glowStep4
	 inv1x1conv 		 L (8, 8) | log_S (8,) | U (8, 8) 		 136
____________________________________________________________________________________________________
glowBlock2/glowStep5
	 ActNorm 		 scale (8,) | shift (8,) 		 16
____________________________________________________________________________________________________
inverse/shift_and_log_scale_res_net_11
	 conv2d_33 		 kernel (3, 3, 8, 64) | bias (64,) 		 4672
	 batch_normalization_22 		 gamma (64,) | beta (64,) 		 128
	 conv2d_34 		 kernel (1, 1, 64, 64) | bias (64,) 		 4160
	 batch_normalization_23 		 gamma (64,) | beta (64,) 		 128
	 conv2d_35 		 kernel (3, 3, 64, 16) | bias (16,) 		 9232
____________________________________________________________________________________________________
glowBlock2/glowStep5
	 inv1x1conv 		 L (8, 8) | log_S (8,) | U (8, 8) 		 136
====================================================================================================
Total Trainable Variables:  179496
Start  Training on 500 epochs
Epoch 000: Loss: 420718521090048.000
Epoch 001: Loss: 4372340146176.000
Epoch 002: Loss: 2469532270592.000
Epoch 003: Loss: 1521902419968.000
Epoch 004: Loss: 1085615570944.000
Epoch 005: Loss: 840025505792.000
Epoch 006: Loss: 669997989888.000
Epoch 007: Loss: 546793684992.000
Epoch 008: Loss: 452681793536.000
Epoch 009: Loss: 377555025920.000
Epoch 010: Loss: 316925018112.000
Epoch 011: Loss: 266328195072.000
Epoch 012: Loss: 223710052352.000
Epoch 013: Loss: 188644229120.000
Epoch 014: Loss: 159093063680.000
Epoch 015: Loss: 134269100032.000
Epoch 016: Loss: 113107820544.000
Epoch 017: Loss: 95263563776.000
Epoch 018: Loss: 79975235584.000
Epoch 019: Loss: 67067998208.000
Epoch 020: Loss: 55944609792.000
Epoch 021: Loss: 46215471104.000
Epoch 022: Loss: 37476499456.000
Epoch 023: Loss: 27440285696.000
Epoch 024: Loss: 20216436736.000
Epoch 025: Loss: 15841276928.000
Epoch 026: Loss: 12602210304.000
Epoch 027: Loss: 10113110016.000
Epoch 028: Loss: 8139712512.000
Epoch 029: Loss: 6527245824.000
Epoch 030: Loss: 5190682112.000
Epoch 031: Loss: 4082111744.000
Epoch 032: Loss: 3183215616.000
Epoch 033: Loss: 2470408448.000
Epoch 034: Loss: 1920988544.000
Epoch 035: Loss: 1509552256.000
Epoch 036: Loss: 1205533440.000
Epoch 037: Loss: 981063424.000
Epoch 038: Loss: 811592192.000
Epoch 039: Loss: 678627264.000
Epoch 040: Loss: 571459136.000
Epoch 041: Loss: 482411424.000
Epoch 042: Loss: 407868768.000
Epoch 043: Loss: 344395840.000
Epoch 044: Loss: 289770880.000
Epoch 045: Loss: 242636896.000
Epoch 046: Loss: 201948352.000
Epoch 047: Loss: 167026960.000
Epoch 048: Loss: 137222448.000
Epoch 049: Loss: 111847296.000
Epoch 050: Loss: 90409856.000
Epoch 051: Loss: 72428856.000
Epoch 052: Loss: 57513952.000
Epoch 053: Loss: 45373508.000
Epoch 054: Loss: 35729112.000
Epoch 055: Loss: 28273072.000
Epoch 056: Loss: 22546326.000
Epoch 057: Loss: 18099690.000
Epoch 058: Loss: 14614315.000
Epoch 059: Loss: 11811813.000
Epoch 060: Loss: 9549286.000
Epoch 061: Loss: 7704221.000
Epoch 062: Loss: 6221031.000
Epoch 063: Loss: 5022462.000
Epoch 064: Loss: 4062509.250
Epoch 065: Loss: 3278439.250
Epoch 066: Loss: 2650275.250
Epoch 067: Loss: 2135541.250
Epoch 068: Loss: 1712671.375
Epoch 069: Loss: 1362193.375
Epoch 070: Loss: 1067215.875
Epoch 071: Loss: 838149.000
Epoch 072: Loss: 669462.000
Epoch 073: Loss: 535757.250
Epoch 074: Loss: 430471.906
Epoch 075: Loss: 354739.375
Epoch 076: Loss: 287676.500
Epoch 077: Loss: 234775.453
Epoch 078: Loss: 195623.781
Epoch 079: Loss: 160058.281
Epoch 080: Loss: 127800.539
Epoch 081: Loss: 110709.297
Epoch 082: Loss: 114458.203
Epoch 083: Loss: 454296.500
Epoch 084: Loss: 85131.391
Epoch 085: Loss: 58815.246
Epoch 086: Loss: 45401.152
Epoch 087: Loss: 36328.082
Epoch 088: Loss: 28960.451
Epoch 089: Loss: 23058.729
Epoch 090: Loss: 23630.104
Epoch 091: Loss: 14399.200
Epoch 092: Loss: 13213.070
Epoch 093: Loss: 9585.387
Epoch 094: Loss: 11391.941
Epoch 095: Loss: 5705.960
Epoch 096: Loss: 6374.978
Epoch 097: Loss: 4806.175
Epoch 098: Loss: 2682.384
Epoch 099: Loss: 4797.588
Epoch 100: Loss: 1298.303
Epoch 101: Loss: 5058.615
Epoch 102: Loss: 730.925
Epoch 103: Loss: 212.150
Epoch 104: Loss: 42749.754
Epoch 105: Loss: 5209.085
Epoch 106: Loss: 1294.701
Epoch 107: Loss: 399.760
Epoch 108: Loss: -31.929
Epoch 109: Loss: -292.709
Epoch 110: Loss: -477.885
Epoch 111: Loss: -629.785
Epoch 112: Loss: -773.776
Epoch 113: Loss: -903.088
Epoch 114: Loss: -1027.775
Epoch 115: Loss: -1183.506
Epoch 116: Loss: -1410.469
Epoch 117: Loss: -1601.263
Epoch 118: Loss: -1762.896
Epoch 119: Loss: -1965.344
Epoch 120: Loss: -2119.124
Epoch 121: Loss: -2274.743
Epoch 122: Loss: -2412.809
Epoch 123: Loss: -2484.812
Epoch 124: Loss: -2673.548
Epoch 125: Loss: -2776.852
Epoch 126: Loss: -1464.703
Epoch 127: Loss: -2763.135
Epoch 128: Loss: -2748.179
Epoch 129: Loss: -2974.580
Epoch 130: Loss: -3175.386
Epoch 131: Loss: -3329.268
Epoch 132: Loss: -3274.444
Epoch 133: Loss: -3351.702
Epoch 134: Loss: -3665.898
Epoch 135: Loss: -3775.204
Epoch 136: Loss: -3901.606
Epoch 137: Loss: -3938.379
Epoch 138: Loss: -3742.977
Epoch 139: Loss: -3696.204
Epoch 140: Loss: -3552.595
Epoch 141: Loss: -1876.283
Epoch 142: Loss: -3370.427
Epoch 143: Loss: -3758.341
Epoch 144: Loss: -3900.232
Epoch 145: Loss: -2484.010
Epoch 146: Loss: -3304.106
Epoch 147: Loss: -3427.927
Epoch 148: Loss: -3783.506
Epoch 149: Loss: -3986.905
Epoch 150: Loss: -3611.630
Epoch 151: Loss: -4270.161
Epoch 152: Loss: -4379.539
Epoch 153: Loss: -4271.755
Epoch 154: Loss: -3905.120
Epoch 155: Loss: -4304.533
Epoch 156: Loss: -4314.833
Epoch 157: Loss: -4405.998
Epoch 158: Loss: -4430.404
Epoch 159: Loss: -4777.482
Epoch 160: Loss: -4880.948
Epoch 161: Loss: -4783.388
Epoch 162: Loss: -4990.307
Epoch 163: Loss: -4741.792
Epoch 164: Loss: -4957.778
Epoch 165: Loss: -4981.405
Epoch 166: Loss: -5060.471
Epoch 167: Loss: -5125.614
Epoch 168: Loss: -5128.169
Epoch 169: Loss: -5192.530
Epoch 170: Loss: -5229.628
Epoch 171: Loss: -5212.412
Epoch 172: Loss: -5275.182
Epoch 173: Loss: -3646.521
Epoch 174: Loss: -4561.978
Epoch 175: Loss: -4831.222
Epoch 176: Loss: -4901.198
Epoch 177: Loss: -4862.123
Epoch 178: Loss: -4275.696
Epoch 179: Loss: -4435.889
Epoch 180: Loss: -4938.389
Epoch 181: Loss: -4922.179
Epoch 182: Loss: -4909.370
Epoch 183: Loss: -4975.429
Epoch 184: Loss: -4298.784
Epoch 185: Loss: -4775.360
Epoch 186: Loss: -5027.026
Epoch 187: Loss: -5037.280
Epoch 188: Loss: -5024.290
Epoch 189: Loss: -5074.672
Epoch 190: Loss: -5098.032
Epoch 191: Loss: -5069.532
Epoch 192: Loss: -5178.092
Epoch 193: Loss: -5172.323
Epoch 194: Loss: -4878.981
Epoch 195: Loss: -5056.741
Epoch 196: Loss: -5178.382
Epoch 197: Loss: -5162.430
Epoch 198: Loss: -3920.020
Epoch 199: Loss: -4837.260
Epoch 200: Loss: -5150.306
Epoch 201: Loss: -5189.014
Epoch 202: Loss: -5096.076
Epoch 203: Loss: -5161.482
Epoch 204: Loss: -5050.587
Epoch 205: Loss: -4468.778
Epoch 206: Loss: -4833.076
Epoch 207: Loss: -5171.536
Epoch 208: Loss: -5245.292
Epoch 209: Loss: -5258.532
Epoch 210: Loss: -5240.262
Epoch 211: Loss: -5201.008
Epoch 212: Loss: -5247.262
Epoch 213: Loss: -5292.030
Epoch 214: Loss: -3337.498
Epoch 215: Loss: -4139.150
Epoch 216: Loss: -4546.786
Epoch 217: Loss: -3988.940
Epoch 218: Loss: -4631.237
Epoch 219: Loss: -4713.155
Epoch 220: Loss: -4785.554
Epoch 221: Loss: -4691.922
Epoch 222: Loss: -4433.831
Epoch 223: Loss: -3623.501
Epoch 224: Loss: -4628.232
Epoch 225: Loss: -4750.628
Epoch 226: Loss: -4591.615
Epoch 227: Loss: -4842.432
Epoch 228: Loss: -4868.346
Epoch 229: Loss: -4836.034
Epoch 230: Loss: -4893.484
Epoch 231: Loss: -4386.672
Epoch 232: Loss: -4878.891
Epoch 233: Loss: -4908.386
Epoch 234: Loss: -4584.511
Epoch 235: Loss: -4695.120
Epoch 236: Loss: -4933.964
Epoch 237: Loss: -4932.504
Epoch 238: Loss: -4072.009
Epoch 239: Loss: -4819.227
Epoch 240: Loss: -5126.522
Epoch 241: Loss: -5237.440
Epoch 242: Loss: -5277.993
Epoch 243: Loss: -5177.405
Epoch 244: Loss: -5345.766
Epoch 245: Loss: -5162.522
Epoch 246: Loss: -5377.605
Epoch 247: Loss: -5347.774
Epoch 248: Loss: -5290.831
Epoch 249: Loss: -5331.103
Epoch 250: Loss: -5447.655
Epoch 251: Loss: -4346.551
Epoch 252: Loss: -5231.997
Epoch 253: Loss: -4967.130
Epoch 254: Loss: -5344.733
Epoch 255: Loss: -5395.054
Epoch 256: Loss: -5162.581
Epoch 257: Loss: -5308.431
Epoch 258: Loss: -5422.005
Epoch 259: Loss: -5220.676
Epoch 260: Loss: -5427.646
Epoch 261: Loss: -5458.546
Epoch 262: Loss: -5349.000
Epoch 263: Loss: -5431.025
Epoch 264: Loss: -4488.472
Epoch 265: Loss: -5031.506
Epoch 266: Loss: -5332.128
Epoch 267: Loss: -5432.750
Epoch 268: Loss: -5019.953
Epoch 269: Loss: -5330.025
Epoch 270: Loss: -5377.370
Epoch 271: Loss: -5421.884
Epoch 272: Loss: -5360.711
Epoch 273: Loss: -5487.270
Epoch 274: Loss: -5333.412
Epoch 275: Loss: -5479.368
Epoch 276: Loss: -5445.005
Epoch 277: Loss: -5471.934
Epoch 278: Loss: -5028.219
Epoch 279: Loss: -5015.838
Epoch 280: Loss: -5454.930
Epoch 281: Loss: -5330.658
Epoch 282: Loss: -5471.744
Epoch 283: Loss: -5460.582
Epoch 284: Loss: -5415.395
Epoch 285: Loss: -5388.843
Epoch 286: Loss: -5426.636
Epoch 287: Loss: -5500.213
Epoch 288: Loss: -5245.500
Epoch 289: Loss: -5533.940
Epoch 290: Loss: -5507.827
Epoch 291: Loss: -5441.711
Epoch 292: Loss: -2751.188
Epoch 293: Loss: -3363.863
Epoch 294: Loss: -3728.210
Epoch 295: Loss: -3889.279
Epoch 296: Loss: -3983.819
Epoch 297: Loss: -4046.517
Epoch 298: Loss: -4057.713
Epoch 299: Loss: -4119.919
Epoch 300: Loss: -4154.081
Epoch 301: Loss: -4081.880
Epoch 302: Loss: -4181.591
Epoch 303: Loss: -4238.833
Epoch 304: Loss: -3731.730
Epoch 305: Loss: -4530.197
Epoch 306: Loss: -4331.231
Epoch 307: Loss: -5018.453
Epoch 308: Loss: -5272.027
Epoch 309: Loss: -5340.113
Epoch 310: Loss: -5386.780
Epoch 311: Loss: -4953.183
Epoch 312: Loss: -5287.708
Epoch 313: Loss: -5347.859
Epoch 314: Loss: -5338.306
Epoch 315: Loss: -5109.082
Epoch 316: Loss: -5443.911
Epoch 317: Loss: -5460.198
Epoch 318: Loss: -4942.562
Epoch 319: Loss: -5401.158
Epoch 320: Loss: -5305.522
Epoch 321: Loss: -5454.520
Epoch 322: Loss: -5485.048
Epoch 323: Loss: -4177.465
Epoch 324: Loss: -5013.919
Epoch 325: Loss: -5264.890
Epoch 326: Loss: -5343.689
Epoch 327: Loss: -5393.280
Epoch 328: Loss: -5418.512
Epoch 329: Loss: -5473.655
Epoch 330: Loss: -5443.186
Epoch 331: Loss: -5482.446
Epoch 332: Loss: -5508.732
Epoch 333: Loss: -5530.291
Epoch 334: Loss: -5388.779
Epoch 335: Loss: -5404.140
Epoch 336: Loss: -5524.935
Epoch 337: Loss: -5539.546
Epoch 338: Loss: -5524.500
Epoch 339: Loss: -5589.726
Epoch 340: Loss: -5595.327
Epoch 341: Loss: -4478.827
Epoch 342: Loss: -5467.553
Epoch 343: Loss: -5342.960
Epoch 344: Loss: -5383.469
Epoch 345: Loss: -4614.012
Epoch 346: Loss: -5354.664
Epoch 347: Loss: -5446.470
Epoch 348: Loss: -5490.273
Epoch 349: Loss: -5539.853
Epoch 350: Loss: -5036.990
Epoch 351: Loss: -5022.558
Epoch 352: Loss: -5518.594
Epoch 353: Loss: -5356.680
Epoch 354: Loss: -5550.128
Epoch 355: Loss: -5542.294
Epoch 356: Loss: -5458.268
Epoch 357: Loss: -5550.736
Epoch 358: Loss: -5570.902
Epoch 359: Loss: -5606.715
Epoch 360: Loss: -5577.932
Epoch 361: Loss: -4736.983
Epoch 362: Loss: -5138.750
Epoch 363: Loss: -4426.599
Epoch 364: Loss: -5437.087
Epoch 365: Loss: -5496.710
Epoch 366: Loss: -3376.246
Epoch 367: Loss: -3961.509
Epoch 368: Loss: -4913.988
Epoch 369: Loss: -5213.527
Epoch 370: Loss: -5393.344
Epoch 371: Loss: -5074.036
Epoch 372: Loss: -5220.047
Epoch 373: Loss: -5320.678
Epoch 374: Loss: -4404.825
Epoch 375: Loss: -4645.758
Epoch 376: Loss: -5096.352
Epoch 377: Loss: -4375.796
Epoch 378: Loss: -3063.581
Epoch 379: Loss: -3328.086
Epoch 380: Loss: -3737.839
Epoch 381: Loss: -3987.178
Epoch 382: Loss: -4099.677
Epoch 383: Loss: -4091.191
Epoch 384: Loss: -4003.731
Epoch 385: Loss: -4298.800
Epoch 386: Loss: -4433.178
Epoch 387: Loss: -4476.497
Epoch 388: Loss: -4171.681
Epoch 389: Loss: -4887.209
Epoch 390: Loss: -5047.866
Epoch 391: Loss: -5082.093
Epoch 392: Loss: -5099.185
Epoch 393: Loss: -5092.908
Epoch 394: Loss: -5092.999
Epoch 395: Loss: -5164.750
Epoch 396: Loss: -1455.158
Epoch 397: Loss: -3901.725
Epoch 398: Loss: -4207.218
Epoch 399: Loss: -4479.938
Epoch 400: Loss: -4590.755
Epoch 401: Loss: -4664.512
Epoch 402: Loss: -4629.794
Epoch 403: Loss: -4798.492
Epoch 404: Loss: -4877.768
Epoch 405: Loss: -4879.117
Epoch 406: Loss: -4981.307
Epoch 407: Loss: -4880.890
Epoch 408: Loss: -5062.785
Epoch 409: Loss: -5071.185
Epoch 410: Loss: -4461.551
Epoch 411: Loss: -5041.419
Epoch 412: Loss: -5038.126
Epoch 413: Loss: -5154.710
Epoch 414: Loss: -5089.510
Epoch 415: Loss: -5110.529
Epoch 416: Loss: -4972.901
Epoch 417: Loss: -5032.920
Epoch 418: Loss: -5115.354
Epoch 419: Loss: -5186.365
Epoch 420: Loss: -5181.859
Epoch 421: Loss: -4757.685
Epoch 422: Loss: -5183.761
Epoch 423: Loss: -5142.983
Epoch 424: Loss: -5180.740
Epoch 425: Loss: -3143.934
Epoch 426: Loss: -4668.900
Epoch 427: Loss: -5115.165
Epoch 428: Loss: -5343.025
Epoch 429: Loss: -5407.778
Epoch 430: Loss: -5496.581
Epoch 431: Loss: -5043.582
Epoch 432: Loss: -5022.713
Epoch 433: Loss: -5002.806
Epoch 434: Loss: -4979.499
Epoch 435: Loss: -5415.639
Epoch 436: Loss: -5564.386
Epoch 437: Loss: -5579.143
Epoch 438: Loss: -5557.708
Epoch 439: Loss: -5433.227
Epoch 440: Loss: -5329.429
Epoch 441: Loss: -5470.422
Epoch 442: Loss: -5470.656
Epoch 443: Loss: -5566.966
Epoch 444: Loss: -4814.550
Epoch 445: Loss: -5122.960
Epoch 446: Loss: -5332.925
Epoch 447: Loss: -5400.229
Epoch 448: Loss: -5466.377
Epoch 449: Loss: -4013.337
Epoch 450: Loss: -4804.034
Epoch 451: Loss: -5061.290
Epoch 452: Loss: -5287.203
Epoch 453: Loss: -5603.001
Epoch 454: Loss: -5372.874
Epoch 455: Loss: -3897.562
Epoch 456: Loss: -5012.922
Epoch 457: Loss: -5110.274
Epoch 458: Loss: -4942.668
Epoch 459: Loss: -5160.208
Epoch 460: Loss: -5207.894
Epoch 461: Loss: -5534.443
Epoch 462: Loss: -5299.194
Epoch 463: Loss: -5329.958
Epoch 464: Loss: -5369.963
Epoch 465: Loss: -5598.405
Epoch 466: Loss: -5618.665
Epoch 467: Loss: -5625.258
Epoch 468: Loss: -5316.998
Epoch 469: Loss: -5269.344
Epoch 470: Loss: -5517.886
Epoch 471: Loss: -5433.343
Epoch 472: Loss: -4991.135
Epoch 473: Loss: -5630.989
Epoch 474: Loss: -5641.832
Epoch 475: Loss: -5698.661
Epoch 476: Loss: -5612.364
Epoch 477: Loss: -5680.239
Epoch 478: Loss: -5648.001
Epoch 479: Loss: -5517.333
Epoch 480: Loss: -5333.238
Epoch 481: Loss: -4480.628
Epoch 482: Loss: -5264.995
Epoch 483: Loss: -5484.088
Epoch 484: Loss: -5568.000
Epoch 485: Loss: -4882.576
Epoch 486: Loss: -4235.452
Epoch 487: Loss: -5340.953
Epoch 488: Loss: -5586.565
Epoch 489: Loss: -5680.452
Epoch 490: Loss: -5664.145
Epoch 491: Loss: -5682.326
Epoch 492: Loss: -5119.222
Epoch 493: Loss: -5656.907
Epoch 494: Loss: -5642.258
Epoch 495: Loss: -5709.592
Epoch 496: Loss: -5530.194
Epoch 497: Loss: -5721.089
Epoch 498: Loss: -5646.789
Epoch 499: Loss: -5746.793
Training time:  10497.77  seconds
loss history saved
9 samples saved
